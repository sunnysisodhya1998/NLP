{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a15b29",
   "metadata": {},
   "source": [
    "# Name: Sunny Kumar\n",
    "_____________________________________\n",
    "    \n",
    "        Subject: NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe417ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317f2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448eae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'side',\n",
       " 'of',\n",
       " 'south',\n",
       " 'America']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of south America\"\n",
    "from nltk.tokenize import word_tokenize # Passing the string text into word tokenize for bre\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a225a2",
   "metadata": {},
   "source": [
    "### Finding the frequency count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1656e0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding frequency distinct in the tokens\n",
    "# importing FreqDist library from nltk and passing token into FreqDist\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f77dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Brazil', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('of', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the frequency of to p10 words\n",
    "fdist1=fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedefd8a",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67e01bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing Porterstemmer from nltk library\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf7b082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"dancing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "317256b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# checking for the list of words\n",
    "stm=['waited','waiting','waits']\n",
    "for word in stm:\n",
    "    print(word+\":\"+pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dccd57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n",
      "give:giv\n"
     ]
    }
   ],
   "source": [
    "# importing LancastersStemmer from nltk\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst= LancasterStemmer()\n",
    "stm=[\"giving\",\"given\",\"gave\",\"give\"]\n",
    "for word in stm:\n",
    "    print(word+\":\"+lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05c54d",
   "metadata": {},
   "source": [
    "### Lemmatization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d5f77",
   "metadata": {},
   "source": [
    "**For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas,\n",
    "stemming would cutoff the ‘ing’ part and convert it to a car. Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7746f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks  : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "#importing lemmatizer library from nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(\"rocks  :\",lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\",lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17743d3",
   "metadata": {},
   "source": [
    "**Stop Words**\n",
    "-----------\n",
    "**“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”,\n",
    "“all”. These words do not provide any meaning and are usually removed from texts. We can\n",
    "remove these stop words using nltk library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fe576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['christiano', 'ronaldo', 'was', 'born', 'on', 'february', '5,1989', ',', 'in', 'funchal', ',', 'madeira']\n",
      "['christiano', 'ronaldo', 'born', 'february', '5,1989', ',', 'funchal', ',', 'madeira']\n"
     ]
    }
   ],
   "source": [
    "# importing stopwords from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a=set(stopwords.words(\"english\"))\n",
    "text=\"christiano ronaldo was born on february 5,1989,in Funchal,Madeira\"\n",
    "text1=word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords=[x for x in text1 if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb983efc",
   "metadata": {},
   "source": [
    "### 29-08-2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea498b73",
   "metadata": {},
   "source": [
    "### Parts of Speech tagging(pos):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08bc903",
   "metadata": {},
   "source": [
    "**Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context.**\n",
    "\n",
    "**There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c4c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text=\"vote to choose a particular man or a group(party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex=word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f8b97",
   "metadata": {},
   "source": [
    "### Name entity recognition:\n",
    "**Named entity recognition It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa7ccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,784.0,168.0\" width=\"784px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"8.16327%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Google</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.08163%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.10204%\" x=\"8.16327%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">'s</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">POS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.7143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"21.4286%\" x=\"13.2653%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"23.8095%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CEO</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.9048%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"23.8095%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Sundar</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"61.9048%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Pichai</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.9524%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.9796%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12.2449%\" x=\"34.6939%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">introduced</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.8163%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.10204%\" x=\"46.9388%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.4898%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.10204%\" x=\"52.0408%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">new</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.5918%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"57.1429%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Pixel</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.08163%\" x=\"64.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.3265%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"24.4898%\" x=\"68.3673%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"45.8333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Minnesota</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.9167%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.8333%\" x=\"45.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Roi</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Centre</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.6122%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"92.8571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Event</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.4286%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Google', 'NNP')]), (\"'s\", 'POS'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Google's CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
    "#importing chunk library from nltk\n",
    "\n",
    "from nltk import ne_chunk #tokenize and pos tagging before\n",
    "token=word_tokenize(text)\n",
    "tags=nltk.pos_tag(token)\n",
    "chunk=ne_chunk(tags)\n",
    "chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4e24f",
   "metadata": {},
   "source": [
    "#### Chunking:\n",
    "**Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a512457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text=\"We saw the yellow dog\"\n",
    "token=word_tokenize(text)\n",
    "tags=nltk.pos_tag(token)\n",
    "reg=\"NP:{<DT>?<JJ>*<NN>}\"\n",
    "a=nltk.RegexpParser(reg)\n",
    "result=a.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666c352",
   "metadata": {},
   "source": [
    "**Some terms that will be frequently used are :**\n",
    "\n",
    "**Corpus – Body of text, singular. Corpora is the plural of this.**\n",
    "\n",
    "**Lexicon – Words and their meanings.**\n",
    "\n",
    "**Token – Each “entity” that is a part of whatever was split up based on rules.**\n",
    "\n",
    "**For examples, each word is a token when a sentence is “tokenized” into words. Each sentence can also be a token,**\n",
    "\n",
    "**if you tokenized the sentences out of a paragraph.**\n",
    "\n",
    "**So basically tokenizing involves splitting sentences and words from the body of the text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1785bb0",
   "metadata": {},
   "source": [
    "### 30-08-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3a690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of computer science,  artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with program  readable logical forms), connecting language and machine perception, managing human']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'program', 'readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 6, 'and': 4, ')': 3, 'language': 2, '(': 2, 'concerned': 2, 'with': 2, 'human': 2, 'Natural': 1, 'processing': 1, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
    "text = '''Natural language processing (NLP) is a field of computer science,  artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with program  readable logical forms), connecting language and machine perception, managing human'''\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))\n",
    "token=word_tokenize(text)\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5dca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words=set(stopwords.words('english'))\n",
    "word_tokens=word_tokenize(example_sent)\n",
    "filtered_sentence=[w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32063d",
   "metadata": {},
   "source": [
    "**Different Methods to Perform Tokenization in Python**\n",
    "\n",
    "Tokenization using Python split() Function\n",
    "\n",
    "Tokenization using Regular Expressions\n",
    "\n",
    "Tokenization using NLTK\n",
    "\n",
    "Tokenization using Spacy\n",
    "\n",
    "Tokenization using Keras\n",
    "\n",
    "Tokenization using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73d411d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b871b622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample sentence, showing off the stop words filtration.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sent[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e40de899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=word_tokenize(example_sent)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe12290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b87d6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e93f878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk,re,pprint\n",
    "from nltk import word_tokenize\n",
    "text=nltk.Text(tokens)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24a398ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1042:2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fc87e",
   "metadata": {},
   "source": [
    "### 9 SEPT 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c219c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843d99be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "#find the no of directories\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab9cff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd6951d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d25c85ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61771e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word,sep=\" \",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b962f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI='''According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\n",
    "\n",
    "Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff5e864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "308ca98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdca7037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According',\n",
       " 'to',\n",
       " 'the',\n",
       " 'father',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'John',\n",
       " 'McCarthy',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " '“',\n",
       " 'The',\n",
       " 'science',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'of',\n",
       " 'making',\n",
       " 'intelligent',\n",
       " 'machines',\n",
       " ',',\n",
       " 'especially',\n",
       " 'intelligent',\n",
       " 'computer',\n",
       " 'programs',\n",
       " '”',\n",
       " '.',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'computer',\n",
       " ',',\n",
       " 'a',\n",
       " 'computer-controlled',\n",
       " 'robot',\n",
       " ',',\n",
       " 'or',\n",
       " 'a',\n",
       " 'software',\n",
       " 'think',\n",
       " 'intelligently',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'similar',\n",
       " 'manner',\n",
       " 'the',\n",
       " 'intelligent',\n",
       " 'humans',\n",
       " 'think.AI',\n",
       " 'is',\n",
       " 'accomplished',\n",
       " 'by',\n",
       " 'studying',\n",
       " 'how',\n",
       " 'human',\n",
       " 'brain',\n",
       " 'thinks',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'humans',\n",
       " 'learn',\n",
       " ',',\n",
       " 'decide',\n",
       " ',',\n",
       " 'and',\n",
       " 'work',\n",
       " 'while',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'a',\n",
       " 'problem',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'using',\n",
       " 'the',\n",
       " 'outcomes',\n",
       " 'of',\n",
       " 'this',\n",
       " 'study',\n",
       " 'as',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'of',\n",
       " 'developing',\n",
       " 'intelligent',\n",
       " 'software',\n",
       " 'and',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_token=word_tokenize(AI)\n",
    "AI_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "373c9e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2210d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding th e frequency of each word occurence\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "283b1370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 10, 'a': 6, 'the': 5, 'of': 5, 'and': 5, 'intelligent': 4, 'is': 3, 'to': 2, 'artificial': 2, 'intelligence': 2, ...})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_token:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0794a",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "1.Bigrams-Tokens of two consecutive written words knows as Bigram\n",
    "\n",
    "2.Trigram-Tokens of three consecutive written words known as Trigram\n",
    "\n",
    "3.Ngram- Tokens of any numbers of consecutive written words known as Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cbc559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a976e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'best',\n",
       " 'and',\n",
       " 'most',\n",
       " 'beautiful',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'even',\n",
       " 'touched',\n",
       " 'but',\n",
       " 'must',\n",
       " 'be',\n",
       " 'felt',\n",
       " 'with',\n",
       " 'the',\n",
       " 'heart',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "string='''The best and most beautiful things in the world cannot be seen or even touched but must be felt with the heart.'''\n",
    "quote_token=nltk.word_tokenize(string)\n",
    "quote_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "913aff25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2371e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best'),\n",
       " ('best', 'and'),\n",
       " ('and', 'most'),\n",
       " ('most', 'beautiful'),\n",
       " ('beautiful', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'even'),\n",
       " ('even', 'touched'),\n",
       " ('touched', 'but'),\n",
       " ('but', 'must'),\n",
       " ('must', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'heart'),\n",
       " ('heart', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram\n",
    "quote_bigrams=list(nltk.bigrams(quote_token))\n",
    "quote_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d7243a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57d627a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and'),\n",
       " ('best', 'and', 'most'),\n",
       " ('and', 'most', 'beautiful'),\n",
       " ('most', 'beautiful', 'things'),\n",
       " ('beautiful', 'things', 'in'),\n",
       " ('things', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'can'),\n",
       " ('world', 'can', 'not'),\n",
       " ('can', 'not', 'be'),\n",
       " ('not', 'be', 'seen'),\n",
       " ('be', 'seen', 'or'),\n",
       " ('seen', 'or', 'even'),\n",
       " ('or', 'even', 'touched'),\n",
       " ('even', 'touched', 'but'),\n",
       " ('touched', 'but', 'must'),\n",
       " ('but', 'must', 'be'),\n",
       " ('must', 'be', 'felt'),\n",
       " ('be', 'felt', 'with'),\n",
       " ('felt', 'with', 'the'),\n",
       " ('with', 'the', 'heart'),\n",
       " ('the', 'heart', '.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trigram\n",
    "quote_trigrams=list(nltk.trigrams(quote_token))\n",
    "quote_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b2c37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bf6099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most'),\n",
       " ('best', 'and', 'most', 'beautiful'),\n",
       " ('and', 'most', 'beautiful', 'things'),\n",
       " ('most', 'beautiful', 'things', 'in'),\n",
       " ('beautiful', 'things', 'in', 'the'),\n",
       " ('things', 'in', 'the', 'world'),\n",
       " ('in', 'the', 'world', 'can'),\n",
       " ('the', 'world', 'can', 'not'),\n",
       " ('world', 'can', 'not', 'be'),\n",
       " ('can', 'not', 'be', 'seen'),\n",
       " ('not', 'be', 'seen', 'or'),\n",
       " ('be', 'seen', 'or', 'even'),\n",
       " ('seen', 'or', 'even', 'touched'),\n",
       " ('or', 'even', 'touched', 'but'),\n",
       " ('even', 'touched', 'but', 'must'),\n",
       " ('touched', 'but', 'must', 'be'),\n",
       " ('but', 'must', 'be', 'felt'),\n",
       " ('must', 'be', 'felt', 'with'),\n",
       " ('be', 'felt', 'with', 'the'),\n",
       " ('felt', 'with', 'the', 'heart'),\n",
       " ('with', 'the', 'heart', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n=4\n",
    "quote_ngrams=list(nltk.ngrams(quote_token,4))\n",
    "quote_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6efa2dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9654c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most', 'beautiful', 'things'),\n",
       " ('best', 'and', 'most', 'beautiful', 'things', 'in'),\n",
       " ('and', 'most', 'beautiful', 'things', 'in', 'the'),\n",
       " ('most', 'beautiful', 'things', 'in', 'the', 'world'),\n",
       " ('beautiful', 'things', 'in', 'the', 'world', 'can'),\n",
       " ('things', 'in', 'the', 'world', 'can', 'not'),\n",
       " ('in', 'the', 'world', 'can', 'not', 'be'),\n",
       " ('the', 'world', 'can', 'not', 'be', 'seen'),\n",
       " ('world', 'can', 'not', 'be', 'seen', 'or'),\n",
       " ('can', 'not', 'be', 'seen', 'or', 'even'),\n",
       " ('not', 'be', 'seen', 'or', 'even', 'touched'),\n",
       " ('be', 'seen', 'or', 'even', 'touched', 'but'),\n",
       " ('seen', 'or', 'even', 'touched', 'but', 'must'),\n",
       " ('or', 'even', 'touched', 'but', 'must', 'be'),\n",
       " ('even', 'touched', 'but', 'must', 'be', 'felt'),\n",
       " ('touched', 'but', 'must', 'be', 'felt', 'with'),\n",
       " ('but', 'must', 'be', 'felt', 'with', 'the'),\n",
       " ('must', 'be', 'felt', 'with', 'the', 'heart'),\n",
       " ('be', 'felt', 'with', 'the', 'heart', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n=6\n",
    "quote_ngrams=list(nltk.ngrams(quote_token,6))\n",
    "quote_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6423b8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a18a5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a8ab9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b6ab5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give,:give,\n",
      "given:given\n",
      "giving:give\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"give,\", \"given\", \"giving\", \"gave\"]\n",
    "for word in words_to_stem:\n",
    "    print(word+\":\"+pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "220729dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give,:give,\n",
      "given:given\n",
      "giving:giving\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len = WordNetLemmatizer()\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +word_len.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499a08a",
   "metadata": {},
   "source": [
    "# lemmatization\n",
    "\n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to a car.\n",
    "\n",
    "Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74498ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4aa84b",
   "metadata": {},
   "source": [
    "# stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c31c474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdaea96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e2cce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 10, 'a': 6, 'the': 5, 'of': 5, 'and': 5, 'intelligent': 4, 'is': 3, 'to': 2, 'artificial': 2, 'intelligence': 2, ...})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability  import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in AI_token:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625569d4",
   "metadata": {},
   "source": [
    "# Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fb77be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,:;()[0-9]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8b3ba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation =[]\n",
    "for words in AI_token:\n",
    "    word = punctuation.sub(\"\",word)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)\n",
    "        \n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2208f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c4b1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = \"The US president satays in the WHITE HOUSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5e6b28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('US', 'NNP'),\n",
       " ('president', 'NN'),\n",
       " ('satays', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('WHITE', 'NNP'),\n",
       " ('HOUSE', 'NNP')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tags  = nltk.pos_tag(NE_tokens)\n",
    "NE_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e621f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GSP US/NNP)\n",
      "  president/NN\n",
      "  satays/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc6493d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe7efe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r\"NP:{<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2551bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser=nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95574396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,576.0,168.0\" width=\"576px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"20.8333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">big</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cat</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.4167%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.94444%\" x=\"20.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ate</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"24.3056%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.7778%\" x=\"27.7778%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">little</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35%\" x=\"65%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mouse</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.5%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.94444%\" x=\"55.5556%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">who</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.0278%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.94444%\" x=\"62.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9722%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.72222%\" x=\"69.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">after</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.3056%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.8333%\" x=\"79.1667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">fresh</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.3333%\" x=\"46.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cheese</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.5833%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae7ccd",
   "metadata": {},
   "source": [
    "# UNIT -2\n",
    "# NLP LAB-2\n",
    "Write a program to tokenize non-English Languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4c5017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est u\n"
     ]
    }
   ],
   "source": [
    "mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est u\"\n",
    "print(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bd2c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est u\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "mytext= \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est u\"\n",
    "print(sent_tokenize(mytext, \"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae32be4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original String:\n",
      "NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
      "\n",
      "\n",
      " Sentence-tokenized copy in a list:\n",
      "['NLTK ist Open Source Software.', 'Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.', 'Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \\nabgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.']\n",
      "\n",
      "Read the list:\n",
      "NLTK ist Open Source Software.\n",
      "Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.\n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n"
     ]
    }
   ],
   "source": [
    "text = '''NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
    "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
    "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
    "'''\n",
    "print(\"\\n Original String:\")\n",
    "print(text)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token_text = sent_tokenize(text,language=\"german\")\n",
    "print(\"\\n Sentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "      print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f270857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'ist',\n",
       " 'Open',\n",
       " 'Source',\n",
       " 'Software',\n",
       " '.',\n",
       " 'Der',\n",
       " 'Quellcode',\n",
       " 'wird',\n",
       " 'unter',\n",
       " 'den',\n",
       " 'Bedingungen',\n",
       " 'der',\n",
       " 'Apache',\n",
       " 'License',\n",
       " 'Version',\n",
       " '2.0',\n",
       " 'vertrieben',\n",
       " '.',\n",
       " 'Die',\n",
       " 'Dokumentation',\n",
       " 'wird',\n",
       " 'unter',\n",
       " 'den',\n",
       " 'Bedingungen',\n",
       " 'der',\n",
       " 'Creative',\n",
       " 'Commons-Lizenz',\n",
       " 'Namensnennung',\n",
       " '-',\n",
       " 'Nicht',\n",
       " 'kommerziell',\n",
       " '-',\n",
       " 'Keine',\n",
       " 'abgeleiteten',\n",
       " 'Werke',\n",
       " '3.0',\n",
       " 'in',\n",
       " 'den',\n",
       " 'Vereinigten',\n",
       " 'Staaten',\n",
       " 'verteilt',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_tokens = nltk.word_tokenize(text)\n",
    "quote_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab29699",
   "metadata": {},
   "source": [
    "# 3 Important NLP Libraries for Indian Languages\n",
    "iNLTK- Hindi, Punjabi, Sanskrit, Gujarati, Kannada, Malyalam, Nepali, Odia, Marathi, Bengali,\n",
    "Tamil, Urdu Indic\n",
    "NLP Library- Assamese, Sindhi, Sinhala, Sanskrit, Konkani, Kannada, Telugu\n",
    "\n",
    "StanfordNLP- Many of the above languages\n",
    "\n",
    "iNLTK (Natural Language Toolkit for Indic Languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea6af6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (21.3)\n",
      "Requirement already satisfied: async-timeout>=3.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (3.0.1)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.6.3)\n",
      "Requirement already satisfied: fastai==1.0.57 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.0.57)\n",
      "Requirement already satisfied: typing in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (3.7.4.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (4.9.3)\n",
      "Requirement already satisfied: aiohttp>=3.5.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (3.7.4.post0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (8.2.0)\n",
      "Requirement already satisfied: fastprogress>=0.1.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (1.0.3)\n",
      "Requirement already satisfied: spacy>=2.0.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (3.4.1)\n",
      "Requirement already satisfied: nvidia-ml-py3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (7.352.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (2.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (2.25.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (0.1.97)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from inltk) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fastai==1.0.57->inltk) (1.12.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fastai==1.0.57->inltk) (0.13.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (6.0.2)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (4.0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (1.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (1.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (8.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (2.4.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (56.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (3.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (3.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy>=2.0.18->inltk) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->inltk) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->inltk) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->inltk) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->inltk) (1.26.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->inltk) (2.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->inltk) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->inltk) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->inltk) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->inltk) (2021.1)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler>=0.10->matplotlib->inltk) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy>=2.0.18->inltk) (5.2.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->inltk) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->inltk) (0.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.0.18->inltk) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.0.18->inltk) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy>=2.0.18->inltk) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e22e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्राचीन काल में विक्रमादित्य नाम के एक आदर्श राजा हुआ करते थे।\n",
      "अपने साहस, पराक्रम और शौर्य के लिए  राजा विक्रम मशहूर थे। \n",
      "ऐसा भी कहा जाता है कि राजा विक्रम अपनी प्राजा के जीवन के दुख दर्द जानने के लिए रात्री के पहर में भेष बदल कर नगर में घूमते थे।\n"
     ]
    }
   ],
   "source": [
    "from inltk.inltk import tokenize\n",
    "\n",
    "hindi_text = \"\"\"प्राचीन काल में विक्रमादित्य नाम के एक आदर्श राजा हुआ करते थे।\n",
    "अपने साहस, पराक्रम और शौर्य के लिए  राजा विक्रम मशहूर थे। \n",
    "ऐसा भी कहा जाता है कि राजा विक्रम अपनी प्राजा के जीवन के दुख दर्द जानने के लिए रात्री के पहर में भेष बदल कर नगर में घूमते थे।\"\"\"\n",
    "\n",
    "print(hindi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88af4fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-ea8b68f1926d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(language_code)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \"\"\"\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "from inltk.inltk import setup\n",
    "setup('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62ee175f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁प्राचीन',\n",
       " '▁काल',\n",
       " '▁में',\n",
       " '▁विक्रमादित्य',\n",
       " '▁नाम',\n",
       " '▁के',\n",
       " '▁एक',\n",
       " '▁आदर्श',\n",
       " '▁राजा',\n",
       " '▁हुआ',\n",
       " '▁करते',\n",
       " '▁थे',\n",
       " '।',\n",
       " '▁अपने',\n",
       " '▁साहस',\n",
       " ',',\n",
       " '▁पराक्रम',\n",
       " '▁और',\n",
       " '▁शौर्य',\n",
       " '▁के',\n",
       " '▁लिए',\n",
       " '▁राजा',\n",
       " '▁विक्रम',\n",
       " '▁मशहूर',\n",
       " '▁थे',\n",
       " '।',\n",
       " '▁ऐसा',\n",
       " '▁भी',\n",
       " '▁कहा',\n",
       " '▁जाता',\n",
       " '▁है',\n",
       " '▁कि',\n",
       " '▁राजा',\n",
       " '▁विक्रम',\n",
       " '▁अपनी',\n",
       " '▁प्रा',\n",
       " 'जा',\n",
       " '▁के',\n",
       " '▁जीवन',\n",
       " '▁के',\n",
       " '▁दुख',\n",
       " '▁दर्द',\n",
       " '▁जानने',\n",
       " '▁के',\n",
       " '▁लिए',\n",
       " '▁रात्री',\n",
       " '▁के',\n",
       " '▁पहर',\n",
       " '▁में',\n",
       " '▁भेष',\n",
       " '▁बदल',\n",
       " '▁कर',\n",
       " '▁नगर',\n",
       " '▁में',\n",
       " '▁घूमते',\n",
       " '▁थे',\n",
       " '।']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inltk.inltk import tokenize\n",
    "\n",
    "hindi_text = \"\"\"प्राचीन काल में विक्रमादित्य नाम के एक आदर्श राजा हुआ करते थे।\n",
    "अपने साहस, पराक्रम और शौर्य के लिए  राजा विक्रम मशहूर थे। \n",
    "ऐसा भी कहा जाता है कि राजा विक्रम अपनी प्राजा के जीवन के दुख दर्द जानने के लिए रात्री के पहर में भेष बदल कर नगर में घूमते थे।\"\"\"\n",
    "\n",
    "# tokenize(input text, language code)\n",
    "tokenize(hindi_text, \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e05bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import get_similar_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9496c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.2.0+cu92 (from versions: 1.7.1, 1.7.1+cpu, 1.7.1+cu101, 1.7.1+cu110, 1.8.0, 1.8.0+cpu, 1.8.0+cu101, 1.8.0+cu111, 1.8.1, 1.8.1+cpu, 1.8.1+cu101, 1.8.1+cu102, 1.8.1+cu111, 1.9.0, 1.9.0+cpu, 1.9.0+cu102, 1.9.0+cu111, 1.9.1, 1.9.1+cpu, 1.9.1+cu102, 1.9.1+cu111, 1.10.0, 1.10.0+cpu, 1.10.0+cu102, 1.10.0+cu111, 1.10.0+cu113, 1.10.1, 1.10.1+cpu, 1.10.1+cu102, 1.10.1+cu111, 1.10.1+cu113, 1.10.2, 1.10.2+cpu, 1.10.2+cu102, 1.10.2+cu111, 1.10.2+cu113, 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116)\n",
      "ERROR: No matching distribution found for torch==1.2.0+cu92\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb2cbbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute '_flat_weights_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-454806c06318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get similar sentences to the one given in hindi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_similar_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'मैं आज बहुत खुश हूं'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36mget_similar_sentences\u001b[1;34m(sen, no_of_variations, language_code, degree_of_aug)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLanguageTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0membedding_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embedding_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;31m# get learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mdefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36mget_embedding_vectors\u001b[1;34m(input, language_code)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mdefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'models'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf'{language_code}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mload_learner\u001b[1;34m(path, file, test, **db_kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[0mcb_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cb_state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[0mclas_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cls'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclas_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'callback_fns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#to avoid duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mload_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcb_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\fastai\\text\\learner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model, split_func, clip, alpha, beta, metrics, **learn_kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m                                                      isinstance(data.train_ds.y, LMLabelList)))\n\u001b[0;32m     51\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_class\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlearn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientClipping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups, add_time, silent, cb_fns_registered)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;34m\"Setup path,metrics, callbacks and ensure model directory exists.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;31m# Note: be v. careful before removing this, as 3rd party device types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Flattens params (on CUDA)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1208\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSTM' object has no attribute '_flat_weights_names'"
     ]
    }
   ],
   "source": [
    "# get similar sentences to the one given in hindi\n",
    "output = get_similar_sentences('मैं आज बहुत खुश हूं', 5, 'hi')\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import identify_language\n",
    "\n",
    "identify_language(\"ഒരു നല്ല ദിനം ആശംസിക്കുന്നു \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import setup\n",
    "setup('ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6790d",
   "metadata": {},
   "source": [
    "# new program\n",
    "Synonyms and Antonyms from Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms\n",
    "from nltk. corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "        synonyms.append(lm.name())\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk. corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "        if lm.antonyms():\n",
    "             antonyms.append(lm.antonyms()[0].name())\n",
    "    \n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_similar_sentences('मैं आज बहुत खुश हूं', 5, 'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import tokenize\n",
    "malayalam_text =\"എതിർവർഗ്ഗലൈംഗികത (Heterosexuality), ഉഭയലൈംഗികത/ദ്വി വർഗ്ഗലൈം\n",
    "tokenize(malayalam_text, \"ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import tokenize\n",
    "bengali_text =\"আবহাওয়া চমৎকার\"\n",
    "tokenize(bengali_text, \"bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_text = \"எனக்கு என் குழந்தை ப் பருவம் நினைவிருக்கிறது\"\n",
    "# tokenize(input text, language code)\n",
    "tokenize(tamil_text, \"ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42054e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "  \n",
    "  \n",
    "# Specifying the language for\n",
    "# detection\n",
    "print(detect(\"Geeksforgeeks is a computer science portal for geeks\"))\n",
    "print(detect(\"Geeksforgeeks - это компьютерный портал для гиков\"))\n",
    "print(detect(\"Geeksforgeeks es un portal informático para geeks\"))\n",
    "print(detect(\"Geeksforgeeks是面向极客的计算机科学门户\"))\n",
    "print(detect(\"Geeksforgeeks geeks के लिए एक कंप्यूटर विज्ञान पोर्टल है\"))\n",
    "print(detect(\"Geeksforgeeksは、ギーク向けのコンピューターサイエンスポータルです。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa96606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d3b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b609e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
